{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd056b9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using AI as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic Neural Network from Scratch\n",
        "This notebook implements a simple neural network from scratch using NumPy. It follows the classic tutorial structure: load MNIST, preprocess, define a 1-hidden-layer network (or single logistic unit), train with batch gradient descent, and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "np.random.seed(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load MNIST (via openml) and preprocess\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "X = mnist['data'].astype(np.float32) / 255.0\n",
        "y = mnist['target'].astype(np.int64)\n",
        "\n",
        "# For the very basic example we'll do multiclass with softmax\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=1)\n",
        "\n",
        "# Transpose so each column is one example (n_x, m)\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "m_train = X_train.shape[1]\n",
        "\n",
        "# One-hot encode labels for training (digits 0..9)\n",
        "num_classes = 10\n",
        "Y_train = np.eye(num_classes)[y_train].T  # shape (10, m_train)\n",
        "Y_test = np.eye(num_classes)[y_test].T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec118c1e",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utility functions\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1.0 / (1.0 + np.exp(-Z))\n",
        "\n",
        "def softmax(Z):\n",
        "    # Z: (num_classes, m)\n",
        "    Z_shift = Z - np.max(Z, axis=0, keepdims=True)\n",
        "    expZ = np.exp(Z_shift)\n",
        "    return expZ / np.sum(expZ, axis=0, keepdims=True)\n",
        "\n",
        "def compute_loss(Y, A):\n",
        "    # cross-entropy for multiclass, averaged over m\n",
        "    m = Y.shape[1]\n",
        "    # clip for numerical stability\n",
        "    A = np.clip(A, 1e-12, 1.0 - 1e-12)\n",
        "    loss = - (1.0 / m) * np.sum(Y * np.log(A))\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize parameters for a simple 1-hidden-layer network\n",
        "n_x = X_train.shape[0]\n",
        "n_h = 64\n",
        "n_y = num_classes\n",
        "\n",
        "W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "b1 = np.zeros((n_h, 1))\n",
        "W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "b2 = np.zeros((n_y, 1))\n",
        "\n",
        "learning_rate = 0.5\n",
        "num_epochs = 500\n",
        "\n",
        "m = m_train\n",
        "\n",
        "# Training loop (full-batch gradient descent)\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward\n",
        "    Z1 = W1.dot(X_train) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    # Loss\n",
        "    loss = compute_loss(Y_train, A2)\n",
        "\n",
        "    # Backprop\n",
        "    dZ2 = A2 - Y_train                    # (n_y, m)\n",
        "    dW2 = (1.0 / m) * dZ2.dot(A1.T)\n",
        "    db2 = (1.0 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    dA1 = W2.T.dot(dZ2)\n",
        "    dZ1 = dA1 * A1 * (1 - A1)            # sigmoid derivative\n",
        "    dW1 = (1.0 / m) * dZ1.dot(X_train.T)\n",
        "    db1 = (1.0 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    # Update\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, loss = {loss:.4f}\")\n",
        "\n",
        "print('Training complete, final loss =', loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "Z1_test = W1.dot(X_test) + b1\n",
        "A1_test = sigmoid(Z1_test)\n",
        "Z2_test = W2.dot(A1_test) + b2\n",
        "A2_test = softmax(Z2_test)\n",
        "\n",
        "preds = np.argmax(A2_test, axis=0)\n",
        "true = y_test\n",
        "\n",
        "print('Confusion matrix:')\n",
        "print(confusion_matrix(true, preds))\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(true, preds))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".scratch_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
